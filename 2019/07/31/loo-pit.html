<!DOCTYPE html>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>GSoC blog</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/gsoc2019_blog/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/gsoc2019_blog/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/gsoc2019_blog/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/gsoc2019_blog/assets/css/ie8.css" /><![endif]-->

    <!-- Favicon head tag -->
    <link rel="shortcut icon" type="image/x-icon" href="/gsoc2019_blog/assets/favicon.ico" />
</head>


<body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="/gsoc2019_blog/" class="logo"><strong>GSoC blog</strong> <span></span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		        <li><a href="/gsoc2019_blog/">Home</a></li>
	    	
		
		    
		
		    
		
        
		
		    
		
		    
		
		    
		
		    
		        <li><a href="/gsoc2019_blog/oss.html">Open Source Software</a></li>
		    
		
		    
		        <li><a href="/gsoc2019_blog/about.html">About me</a></li>
		    
		
	</ul>
</nav>



<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>LOO-PIT tutorial</h1>
		</header>
        
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


		<p>
<p>One of the new functionalities I added in ArviZ during my GSoC internship is Leave One Out (LOO) Probability Integral Transform (PIT) marginal posterior predictive checks. You can see <a href="https://arviz-devs.github.io/arviz/examples/plot_loo_pit_ecdf.html">two</a> <a href="https://arviz-devs.github.io/arviz/examples/plot_loo_pit_overlay.html">examples</a> of its usage in the example gallery and also some examples in its <a href="https://arviz-devs.github.io/arviz/generated/arviz.plot_loo_pit.html#arviz.plot_loo_pit">API section</a>. However, these examples are mainly related to the usage of the functionalities, not so much on the usage of LOO-PIT itself nor its interpretability.</p>

<p>I feel that the LOO-PIT algorithm usage and interpretability needs a short summary with examples showing the most common issues found when checking models with LOO-PIT. This tutorial will tackle this issue: how can LOO-PIT be used for model checking and what does it tell us in a practical manner, so we can see firsthand how wrongly specified models cause LOO-PIT values to differ from a uniform distribution. I have included a short description on what is the algorithm doing, however, for a detailed explanation, see:</p>

<ul>
  <li>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 152-153)</li>
</ul>

<p>We will use LOO-PIT checks along with non-marginal posterior predictive checks as implemented in ArviZ. This will allow to see some differences between the two kinds of posterior predictive checks as well as to provide some intuition to cases where one may be best and cases where both are needed.</p>

<p>Here, we will experiment with LOO-PIT using two different models. First an estimation of the mean and standard deviation of a 1D Gaussian Random Variable, and then a 1D linear regression. Afterwards, we will see how to use LOO-PIT checks with multivariate data using as example a multivariate linear regression.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#Introduction">Introducion</a>
    <ol>
      <li><a href="#Probability-integral-transform">Probability Integral Transform</a></li>
      <li><a href="#Leave-One-Out-Cross-Validation">Leave-One-Out Cross-Validation</a></li>
      <li><a href="#LOO-PIT">LOO-PIT</a></li>
    </ol>
  </li>
  <li>
    <p><a href="#Data-generation">Data generation</a></p>
  </li>
  <li><a href="#Unidimensional-Gaussian-variable">Unidimensional Gaussian variable</a>
    <ol>
      <li><a href="#Overdispersion-signs">Overdispertion signs</a></li>
      <li><a href="#Underdispersion-signs">Underdispersion signs</a></li>
      <li><a href="#Bias-signs">Bias signs</a></li>
    </ol>
  </li>
  <li>
    <p><a href="#Linear-regression">Linear Regression</a></p>
  </li>
  <li><a href="#Multivariate-linear-regression">Multivariate linear regression</a></li>
</ol>

<h2 id="introduction">Introduction</h2>

<p>One of the pilars of Bayesian Statistics is working with the posterior distribution of the parameters instead of using point estimates and errors or confidence intervals. We all know how to obtain this posterior given the likelihood, the prior and the , $p(\theta \mid y) = p(y \mid \theta) p(\theta) / p(y)$. In addition, in many cases we are also interested in <strong>the probability of future observations given the observed data</strong> according to our model. This is called posterior predictive, which is calculated integrating out $\theta$:</p>

\[p(y^* | y) = \int p(y^*|\theta) p(\theta|y) d\theta\]

<p>where $y^*$ is the possible unobserved data and $y$ is the observed data. Therefore, if our model is correct, the observed data and the posterior predictive follow the same probability density function (pdf). In order to check if this holds, it is common to perform posterior predictive checks comparing the posterior predictive to the observed data. This can be done directly, comparing the kernel density estimates (KDE) of the observed data and posterior predictive samples, etc. A KDEs is nothing else than an estimation of the pdf of a random variable given a finite number of samples from this random variable.</p>

<p>Another alternative it to perform LOO-PIT checks, which are a kind of marginal posterior predictive checks. Marginal because we compare each observation only with the corresponding posterior predictive samples instead of combining all observations and all posterior predictive samples. As the name indicates, it combines two different concepts, Leave-One-Out Cross-Validation and Probability Integral Transform.</p>

<h3 id="probability-integral-transform">Probability Integral Transform</h3>

<p>Probability Integral Transform stands for the fact that given a random variable $X$, <strong>the random variable $Y = F_X(X) = P(x \leq X)$ is a uniform random variable if the transformation $F_X$ is the Cumulative Density Function</strong> (CDF) of the original random variable $X$.</p>

<p>If instead of $F_X$ we have $n$ samples from $X$, ${x_1, \dots, x_n}$, we can use them to estimate $\hat{F_X}$ and apply it to future $X$ samples ${x^<em>}$. In this case, $\hat{F_X}(x^</em>)$ will be approximately a uniform random variable, converging to an exact uniform variable as $n$ tends to infinity.</p>

<p>The mathematical demonstration can be found on wikipedia itself just googling it. However here, instead of reproducing it I will try to outline the intuition behind this fact. One way to imagine it is with posterior samples from an MCMC run. If we have enough samples, the probability of a new sample falling between the two smallest values will be the same than the probability of a new sample falling inside the two values closest to the median.</p>

<p>This is because around the probability around the smallest values will be lower, but they will be further apart, whereas the probability around the median will be larger but they will be extremely close. These two effect compensate each other and the probability is indeed the same. Thus, the probability is constant independently of the square the new sample would fall in, which is only compatible with a uniform distribution.</p>

<h3 id="leave-one-out-cross-validation">Leave-One-Out Cross-Validation</h3>

<p>Cross-Validation is one way to try to solve the problem with all the <em>future data</em> I have been mentioning so far. We do not have this future data, so how are we supposed to make calculations with it? Cross-Validation solves this problem by dividing the observed data in $K$ subsets, excluding one subset from the data used to fit the model (so it is data unknown to the model, aka future data) and then using this excluded subset as future data. In general, to get better results, this process is preformed $K$ times, excluding one different subset every time.</p>

<p>LOO-CV is one particular case where the number of subsets is equal to the number of observations so that each iteration only one observation is excluded. That is, we <strong>fit the model one time per observation excluding only this one observation</strong>.</p>

<h3 id="loo-pit">LOO-PIT</h3>

<p>LOO-PIT checks consist on checking the PIT using LOO-CV. That is, fit the model on all data but observation $y_i$ (we will refer to this leave one out subset as $y_{-i}$), use this model to estimate the cumulative density function of the posterior predictive and calculate the PIT, $P(y_i &lt; y^* \mid y_{-i}) = \int_{-\infty}^{y_i} p(y^* \mid y_{-i}) dy^*$, of each observation. Then, the KDE of all LOO-PIT values is estimated to see whether or not it is compatible with the LOO-PIT values being draws from a uniform variable.</p>

<h2 id="data-generation">Data generation</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="n">tt</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">'../forty_blog.mplstyle'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
    <span class="n">ax_ppc</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">);</span> <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
    <span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax_ppc</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ecdf</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">],</span> <span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">)):</span>
        <span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"obs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="n">ecdf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
    <span class="n">ax_ppc</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax_ppc</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">ax_ppc</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_obs</span> <span class="o">=</span> <span class="mi">170</span>
<span class="n">mu_normal</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="n">sd_normal</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">data_normal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_normal</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd_normal</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N_obs</span><span class="p">)</span>
<span class="n">a0_lr</span><span class="p">,</span> <span class="n">a1_lr</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3</span>
<span class="n">sd_lr</span> <span class="o">=</span> <span class="mf">1.4</span>
<span class="n">data_x_regression</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">N_obs</span><span class="p">)</span>
<span class="n">data_y_regression</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">a1_lr</span><span class="o">*</span><span class="n">data_x_regression</span><span class="o">+</span><span class="n">a0_lr</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd_lr</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coords_normal</span> <span class="o">=</span> <span class="p">{</span><span class="s">"obs"</span><span class="p">:</span> <span class="p">[</span><span class="s">"observation"</span><span class="p">],</span> <span class="s">"log_likelihood"</span><span class="p">:</span> <span class="p">[</span><span class="s">"observation"</span><span class="p">]}</span>
<span class="n">dims_normal</span> <span class="o">=</span> <span class="p">{</span><span class="s">"observation"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_obs</span><span class="p">)}</span>
<span class="n">coords_regression</span> <span class="o">=</span> <span class="p">{</span><span class="s">"y"</span><span class="p">:</span> <span class="p">[</span><span class="s">"time"</span><span class="p">],</span> <span class="s">"log_likelihood"</span><span class="p">:</span> <span class="p">[</span><span class="s">"time"</span><span class="p">]}</span>
<span class="n">dims_regression</span> <span class="o">=</span> <span class="p">{</span><span class="s">"time"</span><span class="p">:</span> <span class="n">data_x_regression</span><span class="p">}</span>
</code></pre></div></div>

<p>We will now plot the two datsets generated, to give graphical an idea of the data we are working with.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">textsize</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"axes.labelsize"</span><span class="p">]</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_dist</span><span class="p">(</span><span class="n">data_normal</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rug_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"space"</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span> <span class="n">textsize</span><span class="o">=</span><span class="n">textsize</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">data_x_regression</span><span class="p">,</span> <span class="n">data_y_regression</span><span class="p">,</span> <span class="s">"."</span><span class="p">);</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelsize</span><span class="o">=</span><span class="n">textsize</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Gaussian random variable draws"</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Data for linear regression"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_15_0.png" alt="png" /></p>

<h2 id="unidimensional-gaussian-variable">Unidimensional Gaussian variable</h2>
<p>We will start with a model that correctly fits with the data, to show how should both checks look like. Afterwards, we will see cases were these checks deviate from this ideal case and give some hints on how to interpret these deviations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Define priors
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">"sd"</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
                        <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span><span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sd, mu]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5939.69draws/s]
100%|██████████| 2000/2000 [00:01&lt;00:00, 1867.91it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal</span><span class="p">,</span> <span class="s">"Gaussian: Calibrated model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_18_0.png" alt="png" /></p>

<p>To begin with, it can be seen that <strong>the observed KDE is similar to the overlayed posterior predictive KDEs</strong>. The <strong>same happens with the LOO-PIT values</strong>; the LOO-PIT KDE is similar to the overlayed uniform KDEs. Thus, in this first example, similar information can be obteined from their interpretation.</p>

<h3 id="overdispersion-signs">Overdispersion signs</h3>
<p>We will now move to one common mismatch between the model and the observed data. We will perform the same fit as the previous example but fixing the standard deviation of the normal random variable. This is actually not an unrealistic case, as in many cases where the instrument used to measure gives error data in addition to the measure, this error is used to fix the standard deviation.</p>

<p>These two examples show how the LOO-PIT looks like for overdispersed models (i.e. the error is assumed to be larger than what it actually is) and for underdispersed models (i.e. the error is assumed to be smaller than what it really is).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Define priors
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="s">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">sd_normal</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span>
    <span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_normal_overdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6579.35draws/s]
100%|██████████| 2000/2000 [00:00&lt;00:00, 3737.49it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_overdispersed</span><span class="p">,</span> <span class="s">"Gaussian: Overdispersed model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_22_0.png" alt="png" /></p>

<p>In this example of <strong>overdispersed model</strong>, we can see that the posterior predictive checks show that the <strong>observed KDE is narrower than most of the posterior predictive KDEs</strong> and narrower than the mean KDE of the posterior predictive samples. However, there are still some posterior predictive samples whose KDEs are similar to the observed KDE. In the LOO-PIT check though, there is no room for confursion. <strong>No overlayed distribution in as extreme as the LOO-PIT KDE</strong>, and the difference between the Empirical Cumulative Density Function (ECDF) and the ideal uniform CDF lays outside the envelope most of the time.</p>

<h3 id="underdispersion-signs">Underdispersion signs</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Define priors
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"mu"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="s">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="p">.</span><span class="mi">75</span> <span class="o">*</span> <span class="n">sd_normal</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span>
    <span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_normal_underdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6171.88draws/s]
100%|██████████| 2000/2000 [00:00&lt;00:00, 3812.41it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_underdispersed</span><span class="p">,</span> <span class="s">"Gaussian: Underdispersed model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_26_0.png" alt="png" /></p>

<p>Here, the differences are similar to the overdispersed case, modifying overdispersed by underdispersed and inverting the shapes.</p>

<h3 id="bias-signs">Bias signs</h3>

<p>In addition, LOO-PIT checks also show signs of model bias, as shown in the following example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Define priors
</span>    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">"sd"</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="s">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_normal</span> <span class="o">-</span> <span class="n">sd_normal</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_normal</span>
    <span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_normal_bias</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_normal</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_normal</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sd]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6472.58draws/s]
100%|██████████| 2000/2000 [00:01&lt;00:00, 1905.96it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_normal_bias</span><span class="p">,</span> <span class="s">"Gaussian: Biased model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_30_0.png" alt="png" /></p>

<p>It is important to note though, that the LOO-PIT itself already indicates the problem with the model:</p>
<ul>
  <li>a convex KDE shape (inverted-U shape) or an N in the ECDF difference plot is a sign of an overdispersed model</li>
  <li>a concave KDE shape (U shape) or an inverted-N ECDF difference is a sign of underdispersion</li>
  <li>an asymmetrical KDE or ECDF difference is a sign for model bias</li>
</ul>

<p>In general though, we will probably find a combination of all these cases and it may not be straigthforward to interpretate what is wrong with the model using LOO-PIT or posterior predictive KDE checks.</p>

<h2 id="linear-regression">Linear regression</h2>
<p>In the case of a linear regression, the posterior predictive checks direclty do not give us much information, because each datapoint is centered at a different location, so combining them to create a single KDE won’t yield useful results.
It is important to note though, that this is not an issue inherent to the posterior predictive checks, and could be solved by rescaling each observation by substracting the mean and divide by the standard deviation along every observation from the posterior predictive. We will also include an example of this kind of transformation in the last example, but there should not be much to worry about as this improvement is on the ArviZ roadmap.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># model specifications in PyMC3 are wrapped in a with-statement
</span>    <span class="c1"># Define priors
</span>    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">a0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"a0"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span>
                        <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span><span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_lr</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1, a0, sigma]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:01&lt;00:00, 3302.02draws/s]
The acceptance probability does not match the target. It is 0.8878515682888283, but should be close to 0.8. Try to increase the number of tuning steps.
100%|██████████| 2000/2000 [00:01&lt;00:00, 1426.38it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr</span><span class="p">,</span> <span class="s">"Linear Regression: Calibrated model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_34_0.png" alt="png" /></p>

<p>Now let’s see how does introducing some small bias modifies the results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># model specifications in PyMC3 are wrapped in a with-statement
</span>    <span class="c1"># Define priors
</span>    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0_lr</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span>
                        <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span><span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_lr_bias</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1, sigma]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 5403.58draws/s]
100%|██████████| 2000/2000 [00:01&lt;00:00, 1469.81it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr_bias</span><span class="p">,</span> <span class="s">"Linear Regression: Biased model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_37_0.png" alt="png" /></p>

<p>Now the LOO-PIT check is clearly showing signs of bias in the model, whereas due to the lack of rescaling, no bias is seen in the posterior predictive checks.</p>

<p>Finally, let’s combine some bias with overdispersion, to see how is LOO-PIT modified. Moreover, we will rescale the posterior predictive data to see how would rescaling affect the posterior predictive checks.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># model specifications in PyMC3 are wrapped in a with-statement
</span>    <span class="c1"># Define priors
</span>    <span class="n">a1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"a1"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># Define likelihood
</span>    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'obs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">a0_lr</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">*</span> <span class="n">data_x_regression</span><span class="p">,</span>
                        <span class="n">sd</span><span class="o">=</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">sd_lr</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data_y_regression</span><span class="p">)</span>

    <span class="c1"># Inference!
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># draw posterior samples using NUTS sampling
</span>    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_lr_bias_overdispersed</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span>
    <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span>
    <span class="n">coords</span><span class="o">=</span><span class="n">coords_regression</span><span class="p">,</span>
    <span class="n">dims</span><span class="o">=</span><span class="n">dims_regression</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [a1]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00&lt;00:00, 6264.99draws/s]
100%|██████████| 2000/2000 [00:00&lt;00:00, 2140.36it/s]
</code></pre>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_ppc_loopit</span><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="s">"Linear Regression: Biased and oversidpersed model"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_41_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pp_samples</span> <span class="o">=</span> <span class="n">idata_lr_bias_overdispersed</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">obs</span>
<span class="n">obs_samples</span> <span class="o">=</span> <span class="n">idata_lr_bias_overdispersed</span><span class="p">.</span><span class="n">observed_data</span><span class="p">.</span><span class="n">obs</span>
<span class="n">pp_means</span> <span class="o">=</span> <span class="n">pp_samples</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">"chain"</span><span class="p">,</span> <span class="s">"draw"</span><span class="p">))</span>
<span class="n">pp_stds</span> <span class="o">=</span> <span class="n">pp_samples</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">"chain"</span><span class="p">,</span> <span class="s">"draw"</span><span class="p">))</span>
<span class="n">idata_lr_bias_overdispersed</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">"obs_rescaled"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">pp_samples</span> <span class="o">-</span> <span class="n">pp_means</span><span class="p">)</span> <span class="o">/</span> <span class="n">pp_stds</span>
<span class="n">idata_lr_bias_overdispersed</span><span class="p">.</span><span class="n">observed_data</span><span class="p">[</span><span class="s">"obs_rescaled"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs_samples</span> <span class="o">-</span> <span class="n">pp_means</span><span class="p">)</span> <span class="o">/</span> <span class="n">pp_stds</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="s">"obs"</span><span class="p">,</span> <span class="s">"obs_rescaled"</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">ecdf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">)):</span>
        <span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">var</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="n">ecdf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]);</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_lr_bias_overdispersed</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Linear Regression: Rescaling effect</span><span class="se">\n</span><span class="s">Biased and overdispersed model"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"white"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_43_0.png" alt="png" /></p>

<p>As you can see, the posterior predictive check for <code>obs_rescaled</code> does indicate overdispersion and bias of the posterior predictive samples, whereas the one for <code>obs</code> does not, following what we were seeing previously. The LOO-PIT checks do not change one bit however. This is actually a property of the LOO-PIT algorithm. As it is comparing the marginal distributions of the posterior predictive and the observed data using the MCMC samples, any <em>monotonous</em> transformation will not modify its value because it won’t modify the order between the samples. Therefore, if the observed data is larger than 36% of the posterior predictive samples, the rescaling we have done does not modify this fact.</p>

<p>This bring us to the next topic. What if we were sampling a multidimensional gaussian or doing a multivariate linear regression? Then the LOO-PIT cannot be used directly, because the shape of the posterior predictive samples and that of the likelihood is not the same.</p>

<h2 id="multivariate-linear-regression">Multivariate linear regression</h2>
<p>Our last example will be a multivariate linear regression. We have $N=4$ dependent variables ($X$) from which we want to estimate the value of $M=3$ dependent variables ($Y$), which can be modelled as a multivariate normal variable. To get a proper estimate, we perform $N_{obs}$ observations of each variable.</p>

<p>Therefore, our model will be $Y \sim \text{MvNormal}(\mu, \Sigma)$, with the variable $Y \in \mathbb{R}^{N_{obs}\times M}$, its means $\mu \in \mathbb{R}^{N_{obs}\times M}$ and the covariance matrix $\Sigma \in \mathbb{R}^{M \times M}$. $\mu = X B$ is calculated from the dependent variables $X \in \mathbb{R}^{N_{obs}\times N}$ and the regression parameters $B \in \mathbb{R}^{N \times M}$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_obs</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># looped over with k
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># independent variables, looped over with i
</span><span class="n">M</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># dependent variables, looped over with j
</span><span class="n">dims</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"B"</span><span class="p">:</span> <span class="p">[</span><span class="s">"independent_var"</span><span class="p">,</span> <span class="s">"dependent_var"</span><span class="p">],</span>
    <span class="s">"obs"</span><span class="p">:</span> <span class="p">[</span><span class="s">"observation"</span><span class="p">,</span> <span class="s">"dependent_var"</span><span class="p">],</span>
    <span class="s">"log_likelihood"</span><span class="p">:</span> <span class="p">[</span><span class="s">"observation"</span><span class="p">]}</span>
<span class="n">X_obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">N_obs</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]).</span><span class="n">T</span>
<span class="n">B_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="n">true_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                     <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_obs</span><span class="p">,</span> <span class="n">B_true</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">true_cov</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_obs</span><span class="p">)])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Note that we access the distribution for the standard
</span>    <span class="c1"># deviations, and do not create a new random variable.
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">"X"</span><span class="p">,</span> <span class="n">X_obs</span><span class="p">)</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
    <span class="n">packed_chol</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'chol_cov'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">chol</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">expand_packed_triangular</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">packed_chol</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Define a new MvNormal with the given covariance
</span>    <span class="n">B</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"B"</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">tt</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s">"obs"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">idata_mv_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span> <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">posterior_predictive</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="n">dims</span><span class="p">)</span>
</code></pre></div></div>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [B, chol_cov]
Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [01:29&lt;00:00, 44.50draws/s]
100%|██████████| 2000/2000 [00:00&lt;00:00, 2537.79it/s]
</code></pre>

<p>In these model, each observation is multivariate, thus, the shape of the posterior predictive $(N_{chains}, N_{draws}, N_{obs}, M)$ and the shape of the log likelihood $(N_{chains}, N_{draws}, N_{obs})$ are not the same, preventing us from using LOO-PIT checks directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"pp shape: "</span><span class="p">,</span> <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">obs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"log_like shape: "</span><span class="p">,</span> <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<pre><code>pp shape:  (4, 500, 80, 3)
log_like shape:  (4, 500, 80)
</code></pre>

<p>To be able to use LOO-PIT checks, we have to combine the multivariate observations into an scalar value. This is actually applying a Bayesian test function as an observation-wise function. We coulf use the minimum, the mean, the maximum, or any other any scalar function. If $M$ were larger, quantile values could be used, we could also check the LOO-PIT of each dependent variable… It has the same freedom as Bayesian test functions. For more information of Bayesian test functions see:</p>

<ul>
  <li>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian Data Analysis. Chapman &amp; Hall/CRC Press, London, third edition. (p. 146-149)</li>
</ul>

<p>In this case we will calculate perform LOO-PIT checks on each component of th multivariate observations. This would be similar to marginalizing the multidimensional probability density function on this direction of the $M$-D space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"obs_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">obs</span><span class="p">.</span><span class="n">sel</span><span class="p">(</span><span class="n">dependent_var</span><span class="o">=</span><span class="n">j</span><span class="p">)</span>
    <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">observed_data</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">idata_mv_normal</span><span class="p">.</span><span class="n">observed_data</span><span class="p">.</span><span class="n">obs</span><span class="p">.</span><span class="n">sel</span><span class="p">(</span><span class="n">dependent_var</span><span class="o">=</span><span class="n">j</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"obs_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ecdf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">)):</span>
        <span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_mv_normal</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="n">ecdf</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">col</span><span class="p">]);</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">"Multivariate Linear Regression: Marginalizing over components</span><span class="se">\n</span><span class="s">Calibrated model"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"white"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/notebooks/loo-pit/loo-pit-tutorial_files/loo-pit-tutorial_53_0.png" alt="png" /></p>

<p>Now all 3 LOO-PIT checks are different and bring new information to the table. This is because the functions applied to the posterior predictive and observed data samples are not monotonous transformations, they do modify the relative order between the samples giving each its LOO-PIT values. In this case, as expected because the data is simulated and we are using the same model for analysis and for data generation, all 3 LOO-PIT checks look good.</p>

</p>
	</div>
</section>

</div>

    <!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="copyright">
				
				
				
				
				
				
				
				
				<li><a href="https://github.com/OriolAbril" class="icon fa-github" target="_blank"><span class="label">GitHub</span></a></li>
				
				
				
				<li>&copy; GSoC blog Oriol Abril</li>
				<li>Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a></li>
				<li>Jekyll integration: <a href="https://gitlab.com/andrewbanchich" target="_blank">Andrew Banchich</a></li>
			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="/gsoc2019_blog/assets/js/jquery.min.js"></script>
	<script src="/gsoc2019_blog/assets/js/jquery.scrolly.min.js"></script>
	<script src="/gsoc2019_blog/assets/js/jquery.scrollex.min.js"></script>
	<script src="/gsoc2019_blog/assets/js/skel.min.js"></script>
	<script src="/gsoc2019_blog/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="/gsoc2019_blog/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="/gsoc2019_blog/assets/js/main.js"></script>


</body>

</html>
